{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a causal language model from scratch (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: datasets in /root/miniconda3/envs/llm/lib/python3.10/site-packages (3.6.0)\n",
      "Requirement already satisfied: evaluate in /root/miniconda3/envs/llm/lib/python3.10/site-packages (0.4.3)\n",
      "Requirement already satisfied: transformers[sentencepiece] in /root/miniconda3/envs/llm/lib/python3.10/site-packages (4.51.3)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from datasets) (2.2.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from datasets) (0.31.1)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from transformers[sentencepiece]) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.5.3)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.2.0)\n",
      "Requirement already satisfied: protobuf in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from transformers[sentencepiece]) (6.30.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: accelerate in /root/miniconda3/envs/llm/lib/python3.10/site-packages (1.6.0)\n",
      "Requirement already satisfied: dill in /root/miniconda3/envs/llm/lib/python3.10/site-packages (0.3.8)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from accelerate) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from accelerate) (2.7.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from accelerate) (0.31.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.0)\n",
      "Requirement already satisfied: requests in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (1.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from triton==3.3.0->torch>=2.0.0->accelerate) (78.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Hit:1 https://repo.huaweicloud.com/ubuntu jammy InRelease0m\n",
      "Hit:2 https://repo.huaweicloud.com/ubuntu jammy-backports InRelease\n",
      "Hit:3 https://repo.huaweicloud.com/ubuntu jammy-proposed InRelease\n",
      "Hit:4 https://repo.huaweicloud.com/ubuntu jammy-security InRelease\n",
      "Hit:5 https://repo.huaweicloud.com/ubuntu jammy-updates InRelease\n",
      "Reading package lists... Done\u001b[33m\u001b[33m\u001b[33m\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "153 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "git-lfs is already the newest version (3.0.2-1ubuntu0.3).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 153 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets evaluate transformers[sentencepiece]\n",
    "%pip install accelerate dill ipykernel ipywidgets\n",
    "# To run the training on TPU, you will need to uncomment the following line:\n",
    "# !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
    "!apt update\n",
    "!apt install git-lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to setup git, adapt your email and name in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global user.email \"aokimi0@qq.com\"\n",
    "!git config --global user.name \"aokimi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "# os.environ['HF_ENDPOINT'] = \"https://huggingface.co/\"\n",
    "# os.environ['HF_ENDPOINT'] = \"https://hf-mirror.com\"\n",
    "with open(\"config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "os.environ[\"HF_ENDPOINT\"] = config[\"hf_endpoint\"]\n",
    "os.environ[\"HF_HOME\"] = config[\"hf_home\"]\n",
    "token = config[\"hf_token\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a971f112d240f3a1b2daa2c7ff613e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!git config --global credential.helper store\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def any_keyword_in_string(string, keywords):\n",
    "    for keyword in keywords:\n",
    "        if keyword in string:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False True\n"
     ]
    }
   ],
   "source": [
    "filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n",
    "example_1 = \"import numpy as np\"\n",
    "example_2 = \"import pandas as pd\"\n",
    "\n",
    "print(\n",
    "    any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def filter_streaming_dataset(dataset, filters):\n",
    "    filtered_dict = defaultdict(list)\n",
    "    total = 0\n",
    "    for sample in tqdm(iter(dataset)):\n",
    "        total += 1\n",
    "        if any_keyword_in_string(sample[\"content\"], filters):\n",
    "            for k, v in sample.items():\n",
    "                filtered_dict[k].append(v)\n",
    "    print(f\"{len(filtered_dict['content'])/total:.2%} of data after filtering.\")\n",
    "    return Dataset.from_dict(filtered_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will take a very long time to execute, so you should skip it and go to\n",
    "# the next one!\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# split = \"train\"  # \"valid\"\n",
    "# filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n",
    "\n",
    "# data = load_dataset(f\"transformersbook/codeparrot-{split}\", split=split, streaming=True)\n",
    "# filtered_data = filter_streaming_dataset(data, filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
       "        num_rows: 606720\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
       "        num_rows: 3322\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "ds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\n",
    "ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\n",
    "\n",
    "raw_datasets = DatasetDict(\n",
    "    {\n",
    "        \"train\": ds_train,  # .shuffle().select(range(50000)),\n",
    "        \"valid\": ds_valid,  # .shuffle().select(range(500))\n",
    "    }\n",
    ")\n",
    "\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPO_NAME: kmike/scikit-learn\n",
      "PATH: sklearn/utils/__init__.py\n",
      "COPIES: 3\n",
      "SIZE: 10094\n",
      "CONTENT: \"\"\"\n",
      "The :mod:`sklearn.utils` module includes various utilites.\n",
      "\"\"\"\n",
      "\n",
      "from collections import Sequence\n",
      "\n",
      "import numpy as np\n",
      "from scipy.sparse import issparse\n",
      "import warnings\n",
      "\n",
      "from .murmurhash import murm\n",
      "LICENSE: bsd-3-clause\n"
     ]
    }
   ],
   "source": [
    "for key in raw_datasets[\"train\"][0]:\n",
    "    print(f\"{key.upper()}: {raw_datasets['train'][0][key][:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs length: 34\n",
      "Input chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]\n",
      "Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "context_length = 128\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\n",
    "\n",
    "outputs = tokenizer(\n",
    "    raw_datasets[\"train\"][:2][\"content\"],\n",
    "    truncation=True,\n",
    "    max_length=context_length,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_length=True,\n",
    ")\n",
    "\n",
    "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
    "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
    "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 16702061\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 93164\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"content\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "# 保存\n",
    "with open('session.pkl', 'wb') as f:\n",
    "    dill.dump_session(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 恢复\n",
    "# with open('session.pkl', 'rb') as f:\n",
    "    # dill.load_session(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size: 124.2M parameters\n"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([5, 128])\n",
      "attention_mask shape: torch.Size([5, 128])\n",
      "labels shape: torch.Size([5, 128])\n"
     ]
    }
   ],
   "source": [
    "out = data_collator([tokenized_datasets[\"train\"][i] for i in range(5)])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4009/1376499878.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"codeparrot-ds\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=5_000,\n",
    "    logging_steps=5_000,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=5_000,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='8155' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  40/8155 01:45 < 6:13:49, 0.36 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.push_to_hub()\n",
    "import json\n",
    "\n",
    "# 读取 config.json 文件\n",
    "with open(\"config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "token = config[\"hf_token\"]\n",
    "\n",
    "# 用 token 进行后续操作\n",
    "from huggingface_hub import Repository\n",
    "\n",
    "repo = Repository(\n",
    "    local_dir=\"codeparrot-ds\",\n",
    "    clone_from=\"aokimi/codeparrot-ds\",\n",
    "    use_auth_token=token\n",
    ")\n",
    "repo.push_to_hub(commit_message=\"first commit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=\"huggingface-course/codeparrot-ds\", device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"\"\"\\\n",
    "# create some data\n",
    "x = np.random.randn(100)\n",
    "y = np.random.randn(100)\n",
    "\n",
    "# create scatter plot with x, y\n",
    "\"\"\"\n",
    "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"\"\"\\\n",
    "# create some data\n",
    "x = np.random.randn(100)\n",
    "y = np.random.randn(100)\n",
    "\n",
    "# create dataframe from x and y\n",
    "\"\"\"\n",
    "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"\"\"\\\n",
    "# dataframe with profession, income and name\n",
    "df = pd.DataFrame({'profession': x, 'income':y, 'name': z})\n",
    "\n",
    "# calculate the mean income per profession\n",
    "\"\"\"\n",
    "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"\"\"\n",
    "# import random forest regressor from scikit-learn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# fit random forest model with 300 estimators on X, y:\n",
    "\"\"\"\n",
    "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keytoken_ids = []\n",
    "for keyword in [\n",
    "    \"plt\",\n",
    "    \"pd\",\n",
    "    \"sk\",\n",
    "    \"fit\",\n",
    "    \"predict\",\n",
    "    \" plt\",\n",
    "    \" pd\",\n",
    "    \" sk\",\n",
    "    \" fit\",\n",
    "    \" predict\",\n",
    "    \"testtest\",\n",
    "]:\n",
    "    ids = tokenizer([keyword]).input_ids[0]\n",
    "    if len(ids) == 1:\n",
    "        keytoken_ids.append(ids[0])\n",
    "    else:\n",
    "        print(f\"Keyword has not single token: {keyword}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "import torch\n",
    "\n",
    "\n",
    "def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):\n",
    "    # Shift so that tokens < n predict n\n",
    "    shift_labels = inputs[..., 1:].contiguous()\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    # Calculate per-token loss\n",
    "    loss_fct = CrossEntropyLoss(reduce=False)\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    # Resize and average loss per sample\n",
    "    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)\n",
    "    # Calculate and scale weighting\n",
    "    weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(\n",
    "        axis=[0, 2]\n",
    "    )\n",
    "    weights = alpha * (1.0 + weights)\n",
    "    # Calculate weighted average\n",
    "    weighted_loss = (loss_per_sample * weights).mean()\n",
    "    return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "train_dataloader = DataLoader(tokenized_dataset[\"train\"], batch_size=32, shuffle=True)\n",
    "eval_dataloader = DataLoader(tokenized_dataset[\"valid\"], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 0.1\n",
    "\n",
    "\n",
    "def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
    "    params_with_wd, params_without_wd = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if any(nd in n for nd in no_decay):\n",
    "            params_without_wd.append(p)\n",
    "        else:\n",
    "            params_with_wd.append(p)\n",
    "    return [\n",
    "        {\"params\": params_with_wd, \"weight_decay\": weight_decay},\n",
    "        {\"params\": params_without_wd, \"weight_decay\": 0.0},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
    "\n",
    "        losses.append(accelerator.gather(outputs.loss))\n",
    "    loss = torch.mean(torch.cat(losses))\n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    return loss.item(), perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(get_grouped_params(model), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator(fp16=True)\n",
    "\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 1\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=1_000,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import Repository, get_full_repo_name\n",
    "\n",
    "model_name = \"codeparrot-ds-accelerate\"\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"codeparrot-ds-accelerate\"\n",
    "repo = Repository(output_dir, clone_from=repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "gradient_accumulation_steps = 8\n",
    "eval_steps = 5_000\n",
    "\n",
    "model.train()\n",
    "completed_steps = 0\n",
    "for epoch in range(num_train_epochs):\n",
    "    for step, batch in tqdm(\n",
    "        enumerate(train_dataloader, start=1), total=num_training_steps\n",
    "    ):\n",
    "        logits = model(batch[\"input_ids\"]).logits\n",
    "        loss = keytoken_weighted_loss(batch[\"input_ids\"], logits, keytoken_ids)\n",
    "        if step % 100 == 0:\n",
    "            accelerator.print(\n",
    "                {\n",
    "                    \"lr\": get_lr(),\n",
    "                    \"samples\": step * samples_per_step,\n",
    "                    \"steps\": completed_steps,\n",
    "                    \"loss/train\": loss.item() * gradient_accumulation_steps,\n",
    "                }\n",
    "            )\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        accelerator.backward(loss)\n",
    "        if step % gradient_accumulation_steps == 0:\n",
    "            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            completed_steps += 1\n",
    "        if (step % (eval_steps * gradient_accumulation_steps)) == 0:\n",
    "            eval_loss, perplexity = evaluate()\n",
    "            accelerator.print({\"loss/eval\": eval_loss, \"perplexity\": perplexity})\n",
    "            model.train()\n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "            if accelerator.is_main_process:\n",
    "                tokenizer.save_pretrained(output_dir)\n",
    "                repo.push_to_hub(\n",
    "                    commit_message=f\"Training in progress step {step}\", blocking=False\n",
    "                )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Training a causal language model from scratch (PyTorch)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
